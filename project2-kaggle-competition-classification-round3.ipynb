{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":90936,"databundleVersionId":10665762,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### <font style=\"color:blue\">Project 2: Kaggle Competition - Classification</font>\n\n#### Maximum Points: 100\n\n<div>\n    <table>\n        <tr><td><h3>Sr. no.</h3></td> <td><h3>Section</h3></td> <td><h3>Points</h3></td> </tr>\n        <tr><td><h3>1</h3></td> <td><h3>Data Loader</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>2</h3></td> <td><h3>Configuration</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>3</h3></td> <td><h3>Evaluation Metric</h3></td> <td><h3>10</h3></td> </tr>\n        <tr><td><h3>4</h3></td> <td><h3>Train and Validation</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>5</h3></td> <td><h3>Model</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>6</h3></td> <td><h3>Utils</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>7</h3></td> <td><h3>Experiment</h3></td><td><h3>5</h3></td> </tr>\n        <tr><td><h3>8</h3></td> <td><h3>TensorBoard Dev Scalars Log Link</h3></td> <td><h3>5</h3></td> </tr>\n        <tr><td><h3>9</h3></td> <td><h3>Kaggle Profile Link</h3></td> <td><h3>50</h3></td> </tr>\n    </table>\n</div>\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## <font style=\"color:green\"> Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nfrom dataclasses import dataclass\nfrom typing import List, Union, Tuple\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nimport torch.utils as utils\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom torchvision import datasets, transforms\nimport torchvision.models as models\n\nfrom torchmetrics import MeanMetric\nfrom torchmetrics.classification import MulticlassAccuracy\n\nfrom tqdm import tqdm  # For progress bar\n\n# Text formatting\nbold = \"\\033[1m\"\nend = \"\\033[0m\"\n\nplt.style.use('ggplot')\nblock_plot=False\n\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:46.261003Z","iopub.execute_input":"2025-02-17T21:40:46.261917Z","iopub.status.idle":"2025-02-17T21:40:46.270362Z","shell.execute_reply.started":"2025-02-17T21:40:46.261880Z","shell.execute_reply":"2025-02-17T21:40:46.269603Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## <font style=\"color:green\">1. Data Loader [10 Points]</font>\n\nIn this section, you have to write a class or methods, which will be used to get training and validation data loader.\n\nYou need to write a custom dataset class to load data.\n\n**Note; There is   no separate validation data. , You will thus have to create your own validation set, by dividing the train data into train and validation data. Usually, we do 80:20 ratio for train and validation, respectively.**\n\n\nFor example:\n\n```python\nclass KenyanFood13Dataset(Dataset):\n    \"\"\"\n    \n    \"\"\"\n    \n    def __init__(self, *args):\n    ....\n    ...\n    \n    def __getitem__(self, idx):\n    ...\n    ...\n    \n\n```\n\n\n```python\ndef get_data(args1, *args):\n    ....\n    ....\n    return train_loader, test_loader\n```","metadata":{}},{"cell_type":"markdown","source":"### <font style=\"color:purple\">Prepare Data\n\nSplits data into 80/20 Train and Valid datasets and saves them.","metadata":{}},{"cell_type":"code","source":"class KenyanFood13Dataset(Dataset):\n    def __init__(self, csv_file, img_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (str): Path to the CSV file with image IDs and labels.\n            img_dir (str): Directory where images are stored.\n            transform (callable, optional): Transformations to apply to the images.\n        \"\"\"\n        self.data = pd.read_csv(csv_file)  # Load CSV file\n        self.img_dir = img_dir\n        self.transform = transform\n        \n        # Convert class labels to numerical values if they are categorical\n        self.class_to_idx = {label: idx for idx, label in enumerate(sorted(self.data['class'].unique()))}\n        self.data['class_idx'] = self.data['class'].map(self.class_to_idx)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.img_dir, str(self.data.iloc[idx, 0]) + \".jpg\")  # Get image filename from CSV\n        image = Image.open(img_name).convert('RGB')  # Open image\n        \n        label = int(self.data.iloc[idx, 2])  # Get the class index (numerical label)\n        \n        if self.transform:\n            image = self.transform(image)  # Apply transformations\n            \n        return image, label\n\n# Define transformations for images\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize all images to 224x224 (for CNN models)\n    transforms.ToTensor(),  # Convert images to PyTorch tensors\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize pixel values\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:46.285526Z","iopub.execute_input":"2025-02-17T21:40:46.285775Z","iopub.status.idle":"2025-02-17T21:40:46.292755Z","shell.execute_reply.started":"2025-02-17T21:40:46.285756Z","shell.execute_reply":"2025-02-17T21:40:46.292086Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def get_data(csv_file, img_dir, batch_size=32, train_split=0.8, transform=None):\n    \"\"\"\n    Args:\n        csv_file (str): Path to the CSV file containing image filenames and labels.\n        img_dir (str): Path to the directory where images are stored.\n        batch_size (int): Number of samples per batch.\n        train_split (float): Percentage of data to be used for training (default: 80%).\n        transform (callable, optional): Transformations to apply to the images.\n\n    Returns:\n        train_loader (DataLoader): DataLoader for training set.\n        test_loader (DataLoader): DataLoader for test/validation set.\n    \"\"\"\n    # Load the full dataset\n    full_dataset = KenyanFood13Dataset(csv_file=csv_file, img_dir=img_dir, transform=transform)\n\n    # Split dataset into training and testing sets\n    train_size = int(train_split * len(full_dataset))\n    test_size = len(full_dataset) - train_size\n    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n\n    # Create DataLoaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    return train_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:46.293688Z","iopub.execute_input":"2025-02-17T21:40:46.293966Z","iopub.status.idle":"2025-02-17T21:40:46.323071Z","shell.execute_reply.started":"2025-02-17T21:40:46.293938Z","shell.execute_reply":"2025-02-17T21:40:46.322250Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# File paths\ncsv_file = \"/kaggle/input/opencv-pytorch-project-2-classification-round-3/train.csv\"\nimg_dir = \"/kaggle/input/opencv-pytorch-project-2-classification-round-3/images/images\"\n\n# Load DataLoaders\ntrain_loader, test_loader = get_data(csv_file, img_dir, batch_size=32, transform=transform)\n\n# Check a sample batch\nimages, labels = next(iter(train_loader))\nprint(f\"Batch size: {len(images)}, Image shape: {images[0].shape}, Labels: {labels[:5]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:46.324282Z","iopub.execute_input":"2025-02-17T21:40:46.324468Z","iopub.status.idle":"2025-02-17T21:40:47.983446Z","shell.execute_reply.started":"2025-02-17T21:40:46.324447Z","shell.execute_reply":"2025-02-17T21:40:47.982703Z"}},"outputs":[{"name":"stdout","text":"Batch size: 32, Image shape: torch.Size([3, 224, 224]), Labels: tensor([11,  2, 12, 12,  7])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## <font style=\"color:green\">2. Configuration [5 Points]</font>\n\n**Define your configuration here.**\n\nFor example:\n\n\n```python\n@dataclass\nclass TrainingConfiguration:\n    '''\n    Describes configuration of the training process\n    '''\n    batch_size: int = 10 \n    epochs_count: int = 50  \n    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n    log_interval: int = 5  \n    test_interval: int = 1  \n    data_root: str = \"/kaggle/input/opencv-pytorch-project-2-classification-round-3\" \n    num_workers: int = 2  \n    device: str = 'cuda'  \n    \n```","metadata":{}},{"cell_type":"markdown","source":"### <font style=\"color:pink\">System Configuration","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass SystemConfig:\n    \"\"\"\n    Describes the common system setting needed for reproducible training\n    \"\"\"\n\n    seed: int = 42  # A more widely used seed value for better reproducibility\n    cudnn_benchmark_enabled: bool = True  # Keep enabled for performance boost\n    cudnn_deterministic: bool = False  # Set False for better GPU optimization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:47.984794Z","iopub.execute_input":"2025-02-17T21:40:47.985140Z","iopub.status.idle":"2025-02-17T21:40:47.989529Z","shell.execute_reply.started":"2025-02-17T21:40:47.985104Z","shell.execute_reply":"2025-02-17T21:40:47.988746Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### <font style=\"color:pink\">Training Configuration","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass TrainingConfig:\n    \"\"\"\n    Describes configuration of the training process\n    \"\"\"\n\n    num_classes: int = 3\n    batch_size: int = 16  # Increased batch size for stable gradient updates\n    img_size: Tuple = (224, 224)  # Use standard size for better feature extraction\n    epochs_count: int = 30  # Increase epochs for better learning\n    init_learning_rate: float = 0.0005  # Lower learning rate for stable convergence\n    data_root: str = \"/kaggle/input/opencv-pytorch-project-2-classification-round-3\"\n    num_workers: int = 4  # Increase workers for faster data loading\n    device: str = \"cuda\"  # Keep it on GPU\n\n    # For tensorboard logging and saving checkpoints\n    save_model_name: str = \"kenyan_food_classifier.pt\"\n    root_log_dir: str = os.path.join(\"Logs_Checkpoints\", \"Model_logs\")\n    root_checkpoint_dir: str = os.path.join(\"Logs_Checkpoints\", \"Model_checkpoints\")\n\n    # Current log and checkpoint directory.\n    log_dir: str = \"version_1\"  # Update version for tracking improvements\n    checkpoint_dir: str = \"version_1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:47.990511Z","iopub.execute_input":"2025-02-17T21:40:47.990784Z","iopub.status.idle":"2025-02-17T21:40:48.016333Z","shell.execute_reply.started":"2025-02-17T21:40:47.990763Z","shell.execute_reply":"2025-02-17T21:40:48.015543Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### <font style=\"color:pink\">System Setup","metadata":{}},{"cell_type":"code","source":"def setup_system(system_config: SystemConfig) -> None:\n    torch.manual_seed(system_config.seed)\n    if torch.cuda.is_available():\n        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:48.017229Z","iopub.execute_input":"2025-02-17T21:40:48.017541Z","iopub.status.idle":"2025-02-17T21:40:48.032686Z","shell.execute_reply.started":"2025-02-17T21:40:48.017507Z","shell.execute_reply":"2025-02-17T21:40:48.031863Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## <font style=\"color:green\">3. Evaluation Metric [10 Points]</font>\n\n**Define methods or classes that will be used in model evaluation. For example, accuracy, f1-score etc.**","metadata":{}},{"cell_type":"markdown","source":"## <font style=\"color:purple\">Model Evaluator","metadata":{}},{"cell_type":"code","source":"class ModelEvaluator:\n    def __init__(self, model, dataloader, device):\n        \"\"\"\n        Args:\n            model (torch.nn.Module): The trained PyTorch model.\n            dataloader (DataLoader): DataLoader for evaluation dataset.\n            device (str): 'cuda' or 'cpu'.\n        \"\"\"\n        self.model = model.to(device)\n        self.dataloader = dataloader\n        self.device = device\n\n    def evaluate(self):\n        \"\"\"\n        Evaluates the model on the dataset.\n        \n        Returns:\n            dict: Dictionary containing accuracy, precision, recall, F1-score.\n        \"\"\"\n        self.model.eval()  # Set model to evaluation mode\n\n        all_preds = []\n        all_labels = []\n\n        with torch.no_grad():  # Disable gradient computation for evaluation\n            for images, labels in self.dataloader:\n                images, labels = images.to(self.device), labels.to(self.device)\n\n                # Forward pass\n                outputs = self.model(images)\n                _, preds = torch.max(outputs, 1)  # Get predicted class (argmax)\n\n                all_preds.extend(preds.cpu().numpy())  # Convert to NumPy\n                all_labels.extend(labels.cpu().numpy())\n\n        # Compute metrics\n        accuracy = accuracy_score(all_labels, all_preds)\n        precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n        recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n        f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n        conf_matrix = confusion_matrix(all_labels, all_preds)\n\n        results = {\n            \"Accuracy\": accuracy,\n            \"Precision\": precision,\n            \"Recall\": recall,\n            \"F1-score\": f1,\n            \"Confusion Matrix\": conf_matrix\n        }\n\n        return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:48.034798Z","iopub.execute_input":"2025-02-17T21:40:48.035021Z","iopub.status.idle":"2025-02-17T21:40:48.052685Z","shell.execute_reply.started":"2025-02-17T21:40:48.035003Z","shell.execute_reply":"2025-02-17T21:40:48.051851Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">4. Train and Validation [5 Points]</font>\n\n\n**Write the methods or classes to be used for training and validation.**","metadata":{}},{"cell_type":"code","source":"# Training Function\ndef train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device):\n    model.to(device)\n    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n\n        # Training Phase\n        model.train()\n        running_loss, correct, total = 0.0, 0, 0\n\n        for images, labels in tqdm(train_loader, desc=\"Training\"):\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n            running_loss += loss.item()\n\n        train_loss = running_loss / len(train_loader)\n        train_acc = correct / total\n        history[\"train_loss\"].append(train_loss)\n        history[\"train_acc\"].append(train_acc)\n\n        # Validation Phase\n        val_loss, val_acc = validate_model(model, val_loader, criterion, device)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_acc\"].append(val_acc)\n\n        # Step the scheduler after each epoch\n        scheduler.step()\n\n        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\\n\")\n\n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:48.054033Z","iopub.execute_input":"2025-02-17T21:40:48.054232Z","iopub.status.idle":"2025-02-17T21:40:48.070817Z","shell.execute_reply.started":"2025-02-17T21:40:48.054215Z","shell.execute_reply":"2025-02-17T21:40:48.070032Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Validation Function\ndef validate_model(model, val_loader, criterion, device):\n    model.eval()\n    running_loss, correct, total = 0.0, 0, 0\n\n    with torch.no_grad():\n        for images, labels in tqdm(val_loader, desc=\"Validating\"):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n\n            _, preds = torch.max(outputs, 1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    val_loss = running_loss / len(val_loader)\n    val_acc = correct / total\n    return val_loss, val_acc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:48.071867Z","iopub.execute_input":"2025-02-17T21:40:48.072188Z","iopub.status.idle":"2025-02-17T21:40:48.093753Z","shell.execute_reply.started":"2025-02-17T21:40:48.072146Z","shell.execute_reply":"2025-02-17T21:40:48.093171Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Example Use ","metadata":{}},{"cell_type":"code","source":"# Set device\n# = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Define model, loss function, and optimizer\n#model = MyCustomModel()  # Replace with your model\n#criterion = nn.CrossEntropyLoss()\n#optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train and validate the model\n#history = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:48.094446Z","iopub.execute_input":"2025-02-17T21:40:48.094644Z","iopub.status.idle":"2025-02-17T21:40:48.120756Z","shell.execute_reply.started":"2025-02-17T21:40:48.094627Z","shell.execute_reply":"2025-02-17T21:40:48.120141Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### <font style=\"color:lightgreen\">Main Function for Training","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">5. Model [5 Points]</font>\n\n**Define your model in this section.**\n\n**You are allowed to use any pre-trained model.**","metadata":{}},{"cell_type":"markdown","source":"### <font style=\"color:brown\">Models","metadata":{}},{"cell_type":"code","source":"# Define a CNN Model\nclass CustomCNN(nn.Module):\n    def __init__(self, num_classes):\n        super(CustomCNN, self).__init__()\n\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n\n        self.fc_layers = nn.Sequential(\n            nn.Linear(128 * 28 * 28, 512),  # Assuming input size is 224x224\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = torch.flatten(x, start_dim=1)\n        x = self.fc_layers(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:48.121746Z","iopub.execute_input":"2025-02-17T21:40:48.122041Z","iopub.status.idle":"2025-02-17T21:40:48.136349Z","shell.execute_reply.started":"2025-02-17T21:40:48.122008Z","shell.execute_reply":"2025-02-17T21:40:48.135557Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def get_resnet50_model(num_classes, device):\n    \"\"\"\n    Loads a pretrained ResNet50 model for better accuracy.\n    \"\"\"\n    model = models.resnet50(pretrained=True)  # Load pretrained weights\n    num_features = model.fc.in_features  # Get input size of the last layer\n\n    # Replace final classification layer with a new one\n    # Replace the FC layer with a custom classifier\n    model.fc = nn.Sequential(\n        nn.Linear(num_features, 1024),\n        nn.BatchNorm1d(1024),\n        nn.ReLU(),\n        nn.Dropout(0.5),  # High dropout rate since ResNet50 is strong\n        nn.Linear(1024, num_classes)\n    )\n\n    return model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:48.137219Z","iopub.execute_input":"2025-02-17T21:40:48.137414Z","iopub.status.idle":"2025-02-17T21:40:48.159052Z","shell.execute_reply.started":"2025-02-17T21:40:48.137398Z","shell.execute_reply":"2025-02-17T21:40:48.158463Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">6. Utils [5 Points]</font>\n\n**Define those methods or classes, which have  not been covered in the above sections.**","metadata":{}},{"cell_type":"markdown","source":"### <font style=\"color:purple\"> System Config","metadata":{}},{"cell_type":"code","source":"def setup_system(system_config: SystemConfig) -> None:\n    torch.manual_seed(system_config.seed)\n    if torch.cuda.is_available():\n        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:48.159657Z","iopub.execute_input":"2025-02-17T21:40:48.159844Z","iopub.status.idle":"2025-02-17T21:40:48.177559Z","shell.execute_reply.started":"2025-02-17T21:40:48.159828Z","shell.execute_reply":"2025-02-17T21:40:48.176996Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### <font style=\"color:purple\"> Save & Load Model","metadata":{}},{"cell_type":"code","source":"def save_model(model, device, model_dir=\"models\", model_file_name=\"kenyan_food_classifier.pt\"):\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    model_path = os.path.join(model_dir, model_file_name)\n\n    # Make sure you transfer the model to cpu.\n    if device == \"cuda\":\n        model.to(\"cpu\")\n\n    # Save the 'state_dict'\n    torch.save(model.state_dict(), model_path)\n\n    if device == \"cuda\":\n        model.to(\"cuda\")\n\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:48.178266Z","iopub.execute_input":"2025-02-17T21:40:48.178479Z","iopub.status.idle":"2025-02-17T21:40:48.194388Z","shell.execute_reply.started":"2025-02-17T21:40:48.178461Z","shell.execute_reply":"2025-02-17T21:40:48.193848Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def load_model(model, model_dir=\"models\", model_file_name=\"kenyan_food_classifier.pt\", device=torch.device(\"cpu\")):\n    model_path = os.path.join(model_dir, model_file_name)\n\n    # Load model parameters by using 'load_state_dict'.\n    model.load_state_dict(torch.load(model_path, map_location=device))\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:48.195043Z","iopub.execute_input":"2025-02-17T21:40:48.195213Z","iopub.status.idle":"2025-02-17T21:40:48.217369Z","shell.execute_reply.started":"2025-02-17T21:40:48.195198Z","shell.execute_reply":"2025-02-17T21:40:48.216832Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"### <font style=\"color:purple\"> Logging Setup","metadata":{}},{"cell_type":"code","source":"def setup_log_directory(training_config=TrainingConfig()):\n    \"\"\"Tensorboard Log and Model checkpoint directory Setup\"\"\"\n\n    if os.path.isdir(training_config.root_log_dir):\n        # Get all folders numbers in the root_log_dir.\n        folder_numbers = [int(folder.replace(\"version_\", \"\")) for folder in os.listdir(training_config.root_log_dir)]\n\n        # Find the latest version number present in the log_dir\n        last_version_number = max(folder_numbers)\n\n        # New version name\n        version_name = f\"version_{last_version_number + 1}\"\n\n    else:\n        version_name = training_config.log_dir\n\n    # Update the training config default directory.\n    training_config.log_dir = os.path.join(training_config.root_log_dir, version_name)\n    training_config.checkpoint_dir = os.path.join(training_config.root_checkpoint_dir, version_name)\n\n    # Create new directory for saving new experiment version.\n    os.makedirs(training_config.log_dir, exist_ok=True)\n    os.makedirs(training_config.checkpoint_dir, exist_ok=True)\n\n    print(f\"Logging at: {training_config.log_dir}\")\n    print(f\"Model Checkpoint at: {training_config.checkpoint_dir}\")\n\n    return training_config, version_name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:48.218002Z","iopub.execute_input":"2025-02-17T21:40:48.218178Z","iopub.status.idle":"2025-02-17T21:40:48.234536Z","shell.execute_reply.started":"2025-02-17T21:40:48.218163Z","shell.execute_reply":"2025-02-17T21:40:48.233986Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### <font style=\"color:purple\"> Plot Loss and Accuracy\nThe next code cell will focus on developing a function for plotting loss and accuracy graphs. This function is instrumental in visualizing the performance of the deep learning model throughout the training process, providing insights into its learning behavior by displaying trends in loss reduction and accuracy improvement over epochs.","metadata":{}},{"cell_type":"code","source":"def plot_loss_accuracy(\n    train_loss,\n    val_loss,\n    train_acc,\n    val_acc,\n    colors,\n    loss_legend_loc=\"upper center\",\n    acc_legend_loc=\"upper left\",\n    fig_size=(20, 10),\n    sub_plot1=(1, 2, 1),\n    sub_plot2=(1, 2, 2),\n):\n    plt.rcParams[\"figure.figsize\"] = fig_size\n    fig = plt.figure()\n    plt.subplot(sub_plot1[0], sub_plot1[1], sub_plot1[2])\n\n    for i in range(len(train_loss)):\n        x_train = range(len(train_loss[i]))\n        x_val = range(len(val_loss[i]))\n\n        min_train_loss = min(train_loss[i])\n        min_val_loss = min(val_loss[i])\n\n        plt.plot(x_train, train_loss[i], linestyle=\"-\", color=f\"tab:{colors[i]}\", label=f\"TRAIN LOSS ({min_train_loss:.4})\")\n        plt.plot(x_val, val_loss[i], linestyle=\"--\", color=f\"tab:{colors[i]}\", label=f\"VALID LOSS ({min_val_loss:.4})\")\n\n\n    plt.xlabel(\"epoch no.\")\n    plt.ylabel(\"loss\")\n    plt.legend(loc=loss_legend_loc)\n    plt.title(\"Training and Validation Loss\")\n    plt.subplot(sub_plot2[0], sub_plot2[1], sub_plot2[2])\n\n    for i in range(len(train_acc)):\n        x_train = range(len(train_acc[i]))\n        x_val = range(len(val_acc[i]))\n\n        max_train_acc = max(train_acc[i])\n        max_val_acc = max(val_acc[i])\n\n        plt.plot(\n            x_train,\n            train_acc[i],\n            linestyle=\"-\",\n            color=f\"tab:{colors[i]}\",\n            label=f\"TRAIN ACC ({max_train_acc:.4})\",\n        )\n\n        plt.plot(\n            x_val,\n            val_acc[i],\n            linestyle=\"--\",\n            color=f\"tab:{colors[i]}\",\n            label=f\"VALID ACC ({max_val_acc:.4})\",\n        )\n\n\n    plt.xlabel(\"epoch no.\")\n    plt.ylabel(\"accuracy\")\n    plt.legend(loc=acc_legend_loc)\n    plt.title(\"Training and Validation Accuracy\")\n    fig.savefig(\"sample_loss_acc_plot.png\")\n    plt.show()\n\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:48.235285Z","iopub.execute_input":"2025-02-17T21:40:48.235546Z","iopub.status.idle":"2025-02-17T21:40:48.251013Z","shell.execute_reply.started":"2025-02-17T21:40:48.235526Z","shell.execute_reply":"2025-02-17T21:40:48.250406Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### <font style=\"color:purple\">Sample Prediction","metadata":{}},{"cell_type":"code","source":"def prediction(model, device, batch_input):\n    data = batch_input.to(device)\n\n    with torch.no_grad():\n        output = model(data)\n\n    # Score to probability using softmax.\n    prob = F.softmax(output, dim=1)\n\n    # Get the max probability.\n    pred_prob = prob.data.max(dim=1)[0]\n\n    # Get the index of the max probability.\n    pred_index = prob.data.max(dim=1)[1]\n\n    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:48.253610Z","iopub.execute_input":"2025-02-17T21:40:48.253824Z","iopub.status.idle":"2025-02-17T21:40:48.280395Z","shell.execute_reply.started":"2025-02-17T21:40:48.253807Z","shell.execute_reply":"2025-02-17T21:40:48.279819Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### <font style=\"color:purple\">Get Predictions on a Batch","metadata":{}},{"cell_type":"code","source":"def get_sample_prediction(model, data_root, img_size, mean, std):\n    batch_size = 15\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n        num_workers = 8\n    else:\n        device = \"cpu\"\n        num_workers = 2\n\n    # It is important to do model.eval() before prediction.\n    model.eval()\n\n    # Send model to cpu/cuda according to your system configuration.\n    model.to(device)\n\n    # Transformed data\n    valid_dataset_trans = datasets.ImageFolder(root=data_root, transform=image_common_transforms(img_size, mean, std))\n\n    # Original image dataset\n    valid_dataset = datasets.ImageFolder(root=data_root, transform=image_preprocess_transforms(img_size))\n\n    data_len = valid_dataset.__len__()\n\n    interval = int(data_len / batch_size)\n\n    imgs = []\n    inputs = []\n    targets = []\n    for i in range(batch_size):\n        index = i * interval\n        trans_input, target = valid_dataset_trans.__getitem__(index)\n        img, _ = valid_dataset.__getitem__(index)\n\n        imgs.append(img)\n        inputs.append(trans_input)\n        targets.append(target)\n\n    inputs = torch.stack(inputs)\n\n    cls, prob = prediction(model, device, batch_input=inputs)\n\n    plt.style.use(\"default\")\n    plt.rcParams[\"figure.figsize\"] = (15, 9)\n    fig = plt.figure()\n\n    for i, target in enumerate(targets):\n        plt.subplot(3, 5, i + 1)\n        img = transforms.functional.to_pil_image(imgs[i])\n        plt.imshow(img)\n        plt.gca().set_title(f\"P:{valid_dataset.classes[cls[i]]}({prob[i]:.2}), T:{valid_dataset.classes[targets[i]]}\")\n    plt.show()\n\n    return\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:48.281279Z","iopub.execute_input":"2025-02-17T21:40:48.281520Z","iopub.status.idle":"2025-02-17T21:40:48.299920Z","shell.execute_reply.started":"2025-02-17T21:40:48.281500Z","shell.execute_reply":"2025-02-17T21:40:48.299124Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## <font style=\"color:green\">7. Experiment [5 Points]</font>\n\n**Choose your optimizer and LR-scheduler and use the above methods and classes to train your model.**","metadata":{}},{"cell_type":"code","source":"def main(model, summary_writer, scheduler=None, system_config=SystemConfig(), training_config=TrainingConfig(), data_augmentation=True):\n\n    # Setup system configuration.\n    setup_system(system_config)\n    \n    # Set device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n\n    # File paths for dataset\n    csv_file = \"/kaggle/input/opencv-pytorch-project-2-classification-round-3/train.csv\"\n    img_dir = \"/kaggle/input/opencv-pytorch-project-2-classification-round-3/images/images\"\n\n    # Define transformations (ensure this is compatible with your dataset)\n    transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    # Load Data\n    train_loader, val_loader = get_data(csv_file, img_dir, batch_size=32, transform=transform)\n\n    # Get number of classes from CSV\n    num_classes = len(pd.read_csv(csv_file)['class'].unique())\n    \n    # Define Model, Loss, and Optimizer\n    #model = CustomCNN(num_classes=num_classes).to(device)\n    model = get_resnet50_model(num_classes, device)\n    \n    criterion = nn.CrossEntropyLoss()\n    #optimizer = optim.Adam(model.parameters(), lr=0.001)\n    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)  # AdamW is better for generalization\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Reduce LR every 5 epochs\n    \n\n    # Train Model\n    print(\"Starting training...\")\n    history = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=30, device=device)\n\n    # Save Model\n    save_model(model, device)\n\n    return model, history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:40:48.300729Z","iopub.execute_input":"2025-02-17T21:40:48.300944Z","iopub.status.idle":"2025-02-17T21:40:48.319787Z","shell.execute_reply.started":"2025-02-17T21:40:48.300927Z","shell.execute_reply":"2025-02-17T21:40:48.319061Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:olive\">Training","metadata":{}},{"cell_type":"code","source":"model = get_resnet50_model(num_classes = 13, device = \"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(model)\n\ntraining_config = TrainingConfig()\n\n# Model checkpoint log dir setup.\ntraining_config, current_version_name = setup_log_directory(training_config)\n\n# Tensorboard log dir setup.\nsummary_writer = SummaryWriter(training_config.log_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:43:25.765633Z","iopub.execute_input":"2025-02-17T21:43:25.765974Z","iopub.status.idle":"2025-02-17T21:43:27.412310Z","shell.execute_reply.started":"2025-02-17T21:43:25.765947Z","shell.execute_reply":"2025-02-17T21:43:27.411354Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 208MB/s] \n","output_type":"stream"},{"name":"stdout","text":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Sequential(\n    (0): Linear(in_features=2048, out_features=1024, bias=True)\n    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=1024, out_features=13, bias=True)\n  )\n)\nLogging at: Logs_Checkpoints/Model_logs/version_1\nModel Checkpoint at: Logs_Checkpoints/Model_checkpoints/version_1\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"#Define an optimizer before the scheduler\n# Use Advanced Optimizer & Scheduler\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n\nscheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Reduce LR every 10 epochs\n\n# Train and Validate\nmodel, history = main(\n    model,\n    summary_writer=summary_writer,\n    scheduler=scheduler,  #Use learning rate scheduler\n    system_config=SystemConfig(),\n    training_config=training_config,\n    data_augmentation=True,\n)\n\n\n# Extract loss and accuracy history\ntrain_loss = history[\"train_loss\"]\ntrain_acc = history[\"train_acc\"]\nval_loss = history[\"val_loss\"]\nval_acc = history[\"val_acc\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T21:44:07.471481Z","iopub.execute_input":"2025-02-17T21:44:07.471939Z","iopub.status.idle":"2025-02-17T23:27:14.359410Z","shell.execute_reply.started":"2025-02-17T21:44:07.471909Z","shell.execute_reply":"2025-02-17T23:27:14.358674Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\nEpoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [03:47<00:00,  1.39s/it]\nValidating: 100%|██████████| 41/41 [00:48<00:00,  1.17s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7283 | Train Acc: 0.4593\nVal Loss: 1.6350 | Val Acc: 0.4901\n\nEpoch 2/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:49<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5014 | Train Acc: 0.5371\nVal Loss: 1.3866 | Val Acc: 0.5787\n\nEpoch 3/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:56<00:00,  1.08s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3865 | Train Acc: 0.5752\nVal Loss: 1.3313 | Val Acc: 0.5841\n\nEpoch 4/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:49<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3195 | Train Acc: 0.5901\nVal Loss: 1.2978 | Val Acc: 0.6002\n\nEpoch 5/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:49<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.2677 | Train Acc: 0.6071\nVal Loss: 1.3495 | Val Acc: 0.5910\n\nEpoch 6/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:49<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.0839 | Train Acc: 0.6607\nVal Loss: 1.0496 | Val Acc: 0.6751\n\nEpoch 7/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:48<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9880 | Train Acc: 0.6909\nVal Loss: 1.0222 | Val Acc: 0.6804\n\nEpoch 8/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:49<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9436 | Train Acc: 0.6974\nVal Loss: 1.0387 | Val Acc: 0.6667\n\nEpoch 9/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:48<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.9279 | Train Acc: 0.7116\nVal Loss: 1.0201 | Val Acc: 0.6797\n\nEpoch 10/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:47<00:00,  1.02s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8806 | Train Acc: 0.7244\nVal Loss: 0.9744 | Val Acc: 0.6980\n\nEpoch 11/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:48<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8546 | Train Acc: 0.7282\nVal Loss: 1.0028 | Val Acc: 0.6881\n\nEpoch 12/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:50<00:00,  1.04s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8226 | Train Acc: 0.7397\nVal Loss: 0.9881 | Val Acc: 0.6965\n\nEpoch 13/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:49<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8234 | Train Acc: 0.7368\nVal Loss: 0.9518 | Val Acc: 0.6995\n\nEpoch 14/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:48<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8121 | Train Acc: 0.7351\nVal Loss: 0.9839 | Val Acc: 0.6934\n\nEpoch 15/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:48<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8019 | Train Acc: 0.7517\nVal Loss: 0.9924 | Val Acc: 0.6896\n\nEpoch 16/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:49<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8066 | Train Acc: 0.7458\nVal Loss: 0.9645 | Val Acc: 0.7041\n\nEpoch 17/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:49<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7918 | Train Acc: 0.7502\nVal Loss: 0.9857 | Val Acc: 0.6988\n\nEpoch 18/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:49<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:34<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8189 | Train Acc: 0.7389\nVal Loss: 0.9436 | Val Acc: 0.6988\n\nEpoch 19/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:49<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8129 | Train Acc: 0.7412\nVal Loss: 0.9720 | Val Acc: 0.7041\n\nEpoch 20/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:48<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8227 | Train Acc: 0.7391\nVal Loss: 0.9550 | Val Acc: 0.6927\n\nEpoch 21/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:47<00:00,  1.02s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8166 | Train Acc: 0.7406\nVal Loss: 0.9772 | Val Acc: 0.6965\n\nEpoch 22/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:48<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7971 | Train Acc: 0.7517\nVal Loss: 0.9643 | Val Acc: 0.6980\n\nEpoch 23/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:48<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8089 | Train Acc: 0.7471\nVal Loss: 0.9809 | Val Acc: 0.6965\n\nEpoch 24/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:49<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8174 | Train Acc: 0.7454\nVal Loss: 0.9598 | Val Acc: 0.7095\n\nEpoch 25/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:48<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8028 | Train Acc: 0.7418\nVal Loss: 0.9932 | Val Acc: 0.6911\n\nEpoch 26/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:49<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:35<00:00,  1.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7951 | Train Acc: 0.7464\nVal Loss: 1.0154 | Val Acc: 0.6835\n\nEpoch 27/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:51<00:00,  1.05s/it]\nValidating: 100%|██████████| 41/41 [00:40<00:00,  1.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7957 | Train Acc: 0.7534\nVal Loss: 0.9483 | Val Acc: 0.7057\n\nEpoch 28/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [03:00<00:00,  1.10s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8037 | Train Acc: 0.7464\nVal Loss: 0.9835 | Val Acc: 0.6950\n\nEpoch 29/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:49<00:00,  1.03s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8071 | Train Acc: 0.7423\nVal Loss: 0.9450 | Val Acc: 0.7034\n\nEpoch 30/30\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 164/164 [02:51<00:00,  1.05s/it]\nValidating: 100%|██████████| 41/41 [00:33<00:00,  1.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.8099 | Train Acc: 0.7401\nVal Loss: 0.9206 | Val Acc: 0.7209\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## <font style=\"color:olive\">Load Model and Run Inference","metadata":{}},{"cell_type":"code","source":"trained_model = Model_1()\ntrained_model = load_model(\n    trained_model, \n    model_dir=training_config.checkpoint_dir, \n    model_file_name=training_config.save_model_name\n)\n\ntrain_data_path = os.path.join(training_config.data_root, \"train\")\nvalid_data_path = os.path.join(training_config.data_root, \"valid\")\n\nmean, std = get_mean_std(train_data_path, img_size=training_config.img_size)\n\nget_sample_prediction(trained_model, valid_data_path, img_size=training_config.img_size, mean=mean, std=std)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:54:01.922695Z","iopub.execute_input":"2025-02-16T15:54:01.922968Z","iopub.status.idle":"2025-02-16T15:54:02.018166Z","shell.execute_reply.started":"2025-02-16T15:54:01.922949Z","shell.execute_reply":"2025-02-16T15:54:02.016971Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-20-fcc636219886>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-0c528079766c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m trained_model = load_model(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_file_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-fcc636219886>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model, model_dir, model_file_name, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Load model parameters by using 'load_state_dict'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Logs_Checkpoints/Model_checkpoints/version_2/cat_dog_panda_classifier.pt'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'Logs_Checkpoints/Model_checkpoints/version_2/cat_dog_panda_classifier.pt'","output_type":"error"}],"execution_count":37},{"cell_type":"markdown","source":"### <font style=\"color:purple\">Loss and Accuracy Plot","metadata":{}},{"cell_type":"code","source":"plot_loss_accuracy(\n    train_loss=[train_loss],\n    val_loss=[val_loss],\n    train_acc=[train_acc],\n    val_acc=[val_acc],\n    colors=[\"blue\"],\n    loss_legend_loc=\"upper center\",\n    acc_legend_loc=\"upper left\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T15:08:24.145983Z","iopub.execute_input":"2025-02-16T15:08:24.146201Z","iopub.status.idle":"2025-02-16T15:08:24.299844Z","shell.execute_reply.started":"2025-02-16T15:08:24.146183Z","shell.execute_reply":"2025-02-16T15:08:24.298761Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-8b060eadf4de>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m plot_loss_accuracy(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_loss' is not defined"],"ename":"NameError","evalue":"name 'train_loss' is not defined","output_type":"error"}],"execution_count":17},{"cell_type":"markdown","source":"## <font style=\"color:green\">8. TensorBoard Log Link [5 Points]</font>\n\n**Share your TensorBoard scalars logs link here You can also share (not mandatory) your GitHub link, if you have pushed this project in GitHub.**\n\n\nNote: In light of the recent shutdown of tensorboard.dev, we have updated the submission requirements for your project. Instead of sharing a tensorboard.dev link, you are now required to upload your generated TensorBoard event files directly onto the lab. As an alternative, you may also include a screenshot of your TensorBoard output within your Jupyter notebook. This adjustment ensures that your data visualization and model training efforts are thoroughly documented and accessible for evaluation.\n\nYou are also welcome (and encouraged) to utilize alternative logging services like wandB or comet. In such instances, you can easily make your project logs publicly accessible and share the link with others.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]</font>\n\n**Share your Kaggle profile link  with us here to score , points in  the competition.**\n\n**For full points, you need a minimum accuracy of `75%` on the test data. If accuracy is less than `70%`, you gain  no points for this section.**\n\n\n**Submit `submission.csv` (prediction for images in `test.csv`), in the `Submit Predictions` tab in Kaggle, to get evaluated for  this section.**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}